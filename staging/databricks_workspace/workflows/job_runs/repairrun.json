{
 "/api/2.1/jobs/runs/repair": {
  "post": {
   "operationId": "job-runs-repairrun",
   "externalDocs": {
    "url": "https://docs.databricks.com/api/workspace/jobs/repairrun"
   },
   "x-stackQL-resource": "job_runs",
   "x-stackQL-method": "repairrun",
   "x-stackQL-verb": "exec",
   "x-numReqParams": 0,
   "parameters": [],
   "responses": {
    "200": {
     "description": "Request completed successfully.",
     "content": {
      "application/json": {
       "schema": {
        "type": "object",
        "properties": {
         "repair_id": {
          "type": "integer"
         }
        }
       }
      }
     }
    },
    "400": {
     "description": "Request is invalid or malformed."
    },
    "401": {
     "description": "The request does not have valid authentication credentials for the operation."
    },
    "403": {
     "description": "Caller does not have permission to execute the specified operation."
    },
    "500": {
     "description": "Internal error."
    }
   },
   "requestBody": {
    "required": true,
    "content": {
     "application/json": {
      "schema": {
       "type": "object",
       "properties": {
        "run_id": {
         "type": "required"
        },
        "latest_repair_id": {
         "type": "integer",
         "format": "int64"
        },
        "rerun_tasks": {
         "type": "integer",
         "format": "int64"
        },
        "jar_params": {
         "type": "Array of string"
        },
        "python_params": {
         "type": "Array of string"
        },
        "spark_submit_params": {
         "type": "Array of string"
        },
        "dbt_commands": {
         "type": "Array of string"
        },
        "rerun_all_failed_tasks": {
         "type": "Array of string"
        },
        "rerun_dependent_tasks": {
         "type": "boolean"
        },
        "job_parameters": {
         "type": "object"
        },
        "pipeline_params": {
         "type": "object"
        },
        "notebook_params": {
         "type": "object"
        },
        "python_named_params": {
         "type": "object"
        },
        "sql_params": {
         "type": "object"
        }
       },
       "example": {
        "run_id": 455644833,
        "latest_repair_id": 734650698524280,
        "rerun_tasks": [
         "task0",
         "task1"
        ],
        "job_parameters": {
         "property1": "string",
         "property2": "string"
        },
        "pipeline_params": {
         "full_refresh": false
        },
        "jar_params": [
         "john",
         "doe",
         "35"
        ],
        "notebook_params": {
         "age": "35",
         "name": "john doe"
        },
        "python_params": [
         "john doe",
         "35"
        ],
        "spark_submit_params": [
         "--class",
         "org.apache.spark.examples.SparkPi"
        ],
        "python_named_params": {
         "data": "dbfs:/path/to/data.json",
         "name": "task"
        },
        "sql_params": {
         "age": "35",
         "name": "john doe"
        },
        "dbt_commands": [
         "dbt deps",
         "dbt seed",
         "dbt run"
        ],
        "rerun_all_failed_tasks": false,
        "rerun_dependent_tasks": false
       }
      }
     }
    }
   },
   "description": "Re-run one or more tasks. Tasks are re-run as part of the original job run. They use the current job and task settings, and can be viewed in the history for the original job run."
  }
 }
}